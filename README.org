#+TITLE: PyTorch Examples
#+OPTIONS: toc:2

Welcome to my repository of python scripts which demonstrate PyTorch in simple scenarios. This document  serves as the documentation and guide, where I lightly outline each scenario and present increasingly complex scripts which demonstrate PyTorch's tools.

* Linear Regression
- The following section shows PyTorch's use of Gradient Descent to fit a line to noisy data set.
- The standard linear regression naming conventions are used observed  the input data $x$ and $y$, the fit parameter $w$ and bias $b$, and the predicted dependent value,
  $$\large \hat{y} = w x +b.$$
- Each regression is found using the mean squared error (MSE),
  $$\large loss =  \frac{1}{N} \sum ( \hat{y}_i - y_i)^2.$$
- Each epoch moves the parameters such that the MSE($\hat{y},y)$ is minimized. The epoch 0 line in each figure displays the initial values of the parameters.


** Simple Gradient Descent
[[./LR_noDatasetClass.py][Example script]] using PyTorch for partial derivatives within a simple  linear regression on a  data set with normal noise added. This serves as the first step in using PyTorch as it does not employ any of the other PyTorch features which are the subject of the following examples.

Simple gradient the via PyTorch's partial derivative.
#+begin_src python  :results output
  # tells the tree to calculate the partial derivatives of the loss wrt all of the
  #contributing tensors with the "requires_grad = True" in their constructor.
  loss.backward()

  #gradient descent (with different learning rates)
  w.data = w.data - lr*w.grad.data
  b.data = b.data - lr*b.grad.data

  #must zero out the gradient otherwise PyTorch accumulates the gradient.
  w.grad.data.zero_()
  b.grad.data.zero_()
#+end_src

[[./figs/LR_noDatasetClass.png]]

*** Comments
- The optimal learning rate is directly connect to how good the initial guess is and how noisy the data is.
    - If there is a very large loss (error) and a moderate learning rate, the step is possibly too large, leading to an even larger loss and thus an even larger step, etc, until the loss is NA.
- With a single learning rate, the slope learned much faster than the bias.

** Mini-Batch Gradient Descent using Dataset and DataLoader
[[./LR_miniBatch_datasetDataLoader.py][Example script]] using mini-batch gradient descent for linear regression, while also using PyTorch's Dataset and DataLoader features.
#+begin_src python  :results output
class noisyLineData(Dataset):
    def __init__(self, N=100, slope=3, intercept=2, stdDev=100):
        self.x = torch.linspace(-100,100,N)
        self.y = slope*self.x + intercept + np.random.normal(0, stdDev, N) #can use numpy for random

    def __getitem__(self, index):
        return self.x[index], self.y[index]

    def __len__(self):
        return len(self.x)

data = noisyLineData()

trainloader = DataLoader(dataset = data, batch_size = 20)
#+end_src

[[./figs/LR_miniBatch_datasetDataLoader.png]]

*** Comments
- The *Dataset* and *DataLoader* concepts are simple and useful for abstracting out the data.
    - I believe they will be particularly useful when the data is larger we can hold in the machine's memory.
- With the same learning rates as for the full gradient descent, the mini-batch often learned considerably faster than Gradient Descent per epoch.

** Mini-Batch Gradient Descent the full PyTorch Way
[[./LR_miniBatch_PyTorchWay.py][Example script]] of the same thing, now using nn modules for the model and the optim for optimization (the step):
#+begin_src python  :results output
class linear_regression(nn.Module):
    def __init__(self, input_size, output_size):
        #call the super's constructor and use it without having to store it directly.
        super(linear_regression, self).__init__()
        self.linear = nn.Linear(input_size, output_size)

    def forward(self, x):
        """Prediction"""
        return self.linear(x)

criterion = nn.MSELoss()

model = linear_regression(1,1)
model.state_dict()['linear.weight'][0] = 0
model.state_dict()['linear.bias'][0] = 0

optimizer = optim.SGD(model.parameters(), lr = 1e-4)
#+end_src

[[./figs/LR_miniBatch_PyTorchway.png]]

*** Comments
- The optimizer =optim.SGD= works excellently, beating simple mini-batch easily per-epoch.

* Logistic Regression for Linear Classification
We map the out put of a line/plane to [0,1] for classification. To do this, we use the sigmoid function,

$$\Large \sigma(z) = \frac{1}{1+e^{-z}}$$

as the simple binary function flattens the gradient and thus leads to slow learning.
As a prediction we use,
$\hat{h}= 1$ if $\sigma(x) >0.5$ else $\hat{y} =0$.

We then use new loss to reflect the predictions, *Cross Entropy Loss*. #TODO


** Logistic Regression in 1D
[[./LogReg_PyTorch.py][Example script]]
Now we use linear regression and with the sigmoid function to find the line/plane/hyperplane between two classes, here [0,1].

#+begin_src python  :results output
#create noisy data
class NoisyBinaryData(Dataset):
    def __init__(self, N=100, x0=-3, x1=5, stdDev=2):
        xlist = []; ylist = []
        for i in range(N):
            #class 0
            if np.random.rand()<0.5:
                xlist.append(np.random.normal(x0,stdDev))
                ylist.append(0.0)
            #class 1
            else:
                xlist.append(np.random.normal(x1,stdDev))
                ylist.append(1.0)

        self.x = torch.tensor(xlist).view(-1,1)
        self.y = torch.tensor(ylist).view(-1,1)

    def __getitem__(self, index):
        return self.x[index], self.y[index]

    def __len__(self):
        return len(self.x)

np.random.seed(0)
data = NoisyBinaryData()
trainloader = DataLoader(dataset = data, batch_size = 20)

# create my "own" linear regression model
class logistic_regression(nn.Module):
    def __init__(self, input_size, output_size):
        #call the super's constructor and use it without having to store it directly.
        super(logistic_regression, self).__init__()
        self.linear = nn.Linear(input_size, output_size)

    def forward(self, x):
        """Prediction"""
        return torch.sigmoid(self.linear(x))

#+end_src

*** Loss
The loss is changed so we seperate the data, not fit the data each epoch
I first used the Cross entropy loss, but had a problem with NANs.
#+begin_src python  :results output
def criterion(yhat,y):
    out = -1 * torch.mean(y * torch.log(yhat) + (1 - y) * torch.log(1 - yhat))
    return out
#+end_src

PyTorch's BCELoss fixes this issue by setting $log(0) = \infty$. [[https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html][See here]] for more details.
#+begin_src python  :results output
criterion = nn.BCELoss()
#+end_src


[[./figs/LogReg_PyTorch.png]]

*** Comments
- line does not simply separate the data as y = 0.5 would do that and not give any prediction power.


* Softmax Regression


* Neural Network


* Deep Neural Network


* Convolutional Neural Network


* General PyTorch Notes

** Tensors
